{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/recsys-eval/experiments/run_utils.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import tqdm.autonotebook\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import run_utils\n",
    "\n",
    "sys.path.append('../') \n",
    "import reclab\n",
    "\n",
    "from reclab.recommenders import SLIM, EASE\n",
    "from reclab.recommenders import KNNRecommender\n",
    "from reclab.recommenders.cfnade import Cfnade\n",
    "from reclab import data_utils\n",
    "\n",
    "sys.path.append('../tests') \n",
    "import utils\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for metrics\n",
    "\n",
    "def compute_PREC_REC_MAP_NDCG_MRR(N, users, recs, test_ratings):\n",
    "    assert recs.shape[1] >= N\n",
    "    metrics = ['PREC', 'REC', 'MAP', 'NDCG', 'MRR']\n",
    "    res = {key:[] for key in metrics}\n",
    "    test_rating_matrix = np.array(list(test_ratings.keys()))\n",
    "    for user_id, rec in zip(users, recs):\n",
    "        test_matrix = test_rating_matrix[test_rating_matrix[:,0]==user_id,1]\n",
    "        prec, recall, ncdg = precision_recall_ndcg_at_k(N, rec[:N], test_matrix)\n",
    "        MAP, mrr, ncdg = map_mrr_ndcg(rec[:N], test_matrix)\n",
    "        res['PREC'].append(prec)\n",
    "        res['REC'].append(recall)\n",
    "        res['NDCG'].append(ncdg)\n",
    "        res['MAP'].append(MAP)\n",
    "        res['MRR'].append(mrr)\n",
    "    return {key:np.mean(res[key]) for key in metrics}\n",
    "\n",
    "## From \"A troubling analysis\"... \n",
    "### https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/861eafeaba2943458adec22469b147ec492784b6/Conferences/IJCAI/NeuRec_github/eval.py\n",
    "\n",
    "def precision_recall_ndcg_at_k(k, rankedlist, test_matrix):\n",
    "    idcg_k = 0\n",
    "    dcg_k = 0\n",
    "    n_k = k if len(test_matrix) > k else len(test_matrix)\n",
    "    if n_k == 0:\n",
    "        return 0, 0, 0\n",
    "    for i in range(n_k):\n",
    "        idcg_k += 1 / np.log2(i + 2)\n",
    "\n",
    "    b1 = rankedlist\n",
    "    b2 = test_matrix\n",
    "    s2 = set(b2)\n",
    "    hits = [(idx, val) for idx, val in enumerate(b1) if val in s2]\n",
    "    count = len(hits)\n",
    "\n",
    "    for c in range(count):\n",
    "        dcg_k += 1 / np.log2(hits[c][0] + 2)\n",
    "\n",
    "    return float(count / k), float(count / len(test_matrix)), float(dcg_k / idcg_k)\n",
    "\n",
    "\n",
    "def map_mrr_ndcg(rankedlist, test_matrix):\n",
    "    ap = 0\n",
    "    map = 0\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    mrr = 0\n",
    "    if len(test_matrix) == 0:\n",
    "        return 0, 0, 0\n",
    "    for i in range(len(test_matrix)):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    b1 = rankedlist\n",
    "    b2 = test_matrix\n",
    "    s2 = set(b2)\n",
    "    hits = [(idx, val) for idx, val in enumerate(b1) if val in s2]\n",
    "    count = len(hits)\n",
    "\n",
    "    for c in range(count):\n",
    "        ap += (c + 1) / (hits[c][0] + 1)\n",
    "        dcg += 1 / np.log2(hits[c][0] + 2)\n",
    "\n",
    "    if count != 0:\n",
    "        mrr = 1 / (hits[0][0] + 1)\n",
    "\n",
    "    if count != 0:\n",
    "        map = ap / count\n",
    "\n",
    "    return map, mrr, float(dcg / idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r) / all_pos_num\n",
    "\n",
    "def average_precision(r,cut):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r)\n",
    "    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.sum(out)/float(min(cut, np.sum(r)))\n",
    "\n",
    "def test_one_user(recs, training_items, user_pos_test):\n",
    "    if len(user_pos_test) == 0:\n",
    "        return None\n",
    "    r = []\n",
    "    for i in recs:\n",
    "        if i in user_pos_test:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "    \n",
    "    recall_20 = recall_at_k(r, 20, len(user_pos_test))\n",
    "    recall_40 = recall_at_k(r, 40, len(user_pos_test))\n",
    "    recall_60 = recall_at_k(r, 60, len(user_pos_test))\n",
    "    recall_80 = recall_at_k(r, 80, len(user_pos_test))\n",
    "    recall_100 = recall_at_k(r, 100, len(user_pos_test))\n",
    "\n",
    "    ap_20 = average_precision(r,20)\n",
    "    ap_40 = average_precision(r, 40)\n",
    "    ap_60 = average_precision(r, 60)\n",
    "    ap_80 = average_precision(r, 80)\n",
    "    ap_100 = average_precision(r, 100)\n",
    "\n",
    "\n",
    "    return np.array([recall_20,recall_40,recall_60,recall_80,recall_100, ap_20,ap_40,ap_60,ap_80,ap_100])\n",
    "\n",
    "\n",
    "def test(recs, train_ratings, test_ratings, users):\n",
    "    train_items = collections.defaultdict(list)\n",
    "    for uid, iid in train_ratings:\n",
    "        train_items[uid].append(iid)\n",
    "\n",
    "    test_items = collections.defaultdict(list) \n",
    "    for uid, iid in test_ratings:\n",
    "        test_items[uid].append(iid)\n",
    "    result = np.array([0.] * 10)\n",
    "    tot_num = 0\n",
    "    for user_id, rec in zip(users, recs):\n",
    "        res = test_one_user(rec, train_items[user_id], test_items[user_id])\n",
    "        if res is not None:\n",
    "            result += res\n",
    "            tot_num += 1\n",
    "\n",
    "    ret = result / tot_num\n",
    "    return list(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIM\n",
    "\n",
    "In \"A troubling analysis\" (https://arxiv.org/pdf/1911.07698.pdf) Table 12, SLIM achieves the following results on ML 1M.\n",
    "\n",
    "\n",
    "| PREC@5   | REC@5   | MAP@5   | NDCG@5   | MRR@5   | PREC@10   | REC@10   | MAP@10   | NDCG@10   |  MRR@10 |\n",
    "|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n",
    "| 0.4437 |  0.1106 |  0.3692 |  0.1749 |  0.6578 | 0.3813 |  0.1770 |  0.3003 |  0.2321 |  0.667 |\n",
    "\n",
    "\n",
    "In this paper, the dataset is converted into a implicit dataset, so ratings are either 1 or 0. Evaulation was performed by averaging over five different 80/20 train/test splits. (We will just look at a single split below).\n",
    "\n",
    " The [hyperparameters](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/861eafeaba2943458adec22469b147ec492784b6/DL_Evaluation_TOIS_Additional_material.pdf) are set as `l1_ratio=1.89e-5` and `alpha=0.049`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = data_utils.read_dataset('ml-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ratings.keys():\n",
    "    ratings[key] = (1, ratings[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contexts = collections.OrderedDict([(user_id, np.zeros(0)) for user_id in users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = data_utils.split_ratings(ratings, 0.8, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = SLIM(alpha=0.049, l1_ratio=1.89e-5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommender.reset(users, items, train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs, _ = recommender.recommend(all_contexts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@5: {'PREC': 0.3241721854304636, 'REC': 0.06738210300791221, 'MAP': 0.43651995952906547, 'NDCG': 0.10395269380895251, 'MRR': 0.45423013245033117}\n",
      "@10: {'PREC': 0.3814238410596027, 'REC': 0.17422638615796424, 'MAP': 0.43883045470150284, 'NDCG': 0.18934677631282193, 'MRR': 0.47939562966466936}\n"
     ]
    }
   ],
   "source": [
    "for N in [5, 10]:\n",
    "    res = compute_PREC_REC_MAP_NDCG_MRR(N, users, recs, test_ratings)\n",
    "    print('@{}:'.format(N), res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASE\n",
    "\n",
    "In \"A troubling analysis\" (https://arxiv.org/pdf/1911.07698.pdf), EASE achieves the following results on ML 1M\n",
    "\n",
    "\n",
    "\n",
    "| PREC@5   | REC@5   | MAP@5   | NDCG@5   | MRR@5   | PREC@10   | REC@10   | MAP@10   | NDCG@10   |  MRR@10 |\n",
    "|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n",
    "| 0.4360  | 0.1073  | 0.3608  | 0.1697  | 0.6475 | 0.3745  | 0.1731  | 0.2923  | 0.2259  | 0.65| \n",
    " \n",
    "In this paper, the dataset is converted into a implicit dataset, so ratings are either 1 or 0. Evaulation was performed by averaging over five different 80/20 train/test splits. (We will just look at a single split below).\n",
    "\n",
    "The [hyperparameters](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/861eafeaba2943458adec22469b147ec492784b6/DL_Evaluation_TOIS_Additional_material.pdf) are set as `lam=1.25e3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = data_utils.read_dataset('ml-1m')\n",
    "for key in ratings.keys():\n",
    "    ratings[key] = (1, ratings[key][1])\n",
    "all_contexts = collections.OrderedDict([(user_id, np.zeros(0)) for user_id in users])\n",
    "train_ratings, test_ratings = data_utils.split_ratings(ratings, 0.8, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = EASE(lam=1.25e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "recommender.reset(users, items, train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs, _ = recommender.recommend(all_contexts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@5: {'PREC': 0.32466887417218543, 'REC': 0.06821147096556184, 'MAP': 0.4329348325974981, 'NDCG': 0.10444392654073191, 'MRR': 0.4466197571743929}\n",
      "@10: {'PREC': 0.38415562913907286, 'REC': 0.17628998806236673, 'MAP': 0.4373985546772671, 'NDCG': 0.19097229149913342, 'MRR': 0.4719510801009146}\n"
     ]
    }
   ],
   "source": [
    "for N in [5, 10]:\n",
    "    res = compute_PREC_REC_MAP_NDCG_MRR(N, users, recs, test_ratings)\n",
    "    print('@{}:'.format(N), res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UserKNN cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Surprise repo changing the benchmarking script (https://github.com/NicolasHug/Surprise/blob/master/examples/benchmark.py) on KNNWithMeans to use cosine similarity leads to an RMSE of 0.942 on MovieLens 1M.\n",
    "\n",
    "\n",
    "The [hyperparameters](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans) (Table 42) are set as `topK=40`, `shrink=0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[autoreload of reclab.recommenders.top_pop failed: Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "users, items, ratings = data_utils.read_dataset('ml-1m')\n",
    "all_contexts = collections.OrderedDict([(user_id, np.zeros(0)) for user_id in users])\n",
    "train_ratings, test_ratings = data_utils.split_ratings(ratings, 0.8, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = KNNRecommender(shrinkage=0, neighborhood_size=40, user_based=True, use_means=True, use_content=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.reset(users, items, train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = recommender.dense_predictions\n",
    "t = [(uid, iid, np.zeros(0)) for uid, iid in test_ratings]\n",
    "preds = recommender.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.9458989545474523\n"
     ]
    }
   ],
   "source": [
    "tot = 0.0\n",
    "num = 0.0\n",
    "for (uid, iid, _), pred in zip(t, preds):\n",
    "    tot += (test_ratings[uid, iid][0] - pred) **2\n",
    "    num += 1\n",
    "print(\"RMSE is\", np.sqrt(tot / num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF-NADE\n",
    "\n",
    "In \"A Neural Autoregressive Approach to Collaborative Filtering\" (https://arxiv.org/pdf/1605.09477.pdf) Figure 1, CF-Nade (item-based, with the ordinal cost) achieves the following results on ML 1M:\n",
    "\n",
    "with $\\lambda = 1$, CF-NADE RMSE = 0.836$\\sim$0.837 \n",
    "\n",
    "\n",
    "10 percent of the ratings in each of these datasets are randomly selected as the test set,\n",
    "leaving the remaining 90 percent of the ratings as the training set. Among the ratings in the training set, 5 percent are used as validation set. We use a default rating of 3 for items without training observations. Prediction error is measured by Root Mean Squared Error (RMSE).\n",
    "The authors report the average RMSE on test set over 5 different splits.\n",
    "\n",
    "The hyperparameters: \n",
    "\n",
    "The configuration of the experiments is as follows. We use\n",
    "a single hidden layer architecture and the number of hidden units is set to 500, with default parameters (b1 = 0.1,b2 = 0.001 and $\\epsilon$ = 10−8, for Adam) are utilized to optimize the cost function in Equation 19. The learning rate is set to 0.001 , the weight decay is set to 0.015 and we use the tanh activation function. Batch size was set to 512.\n",
    "(https://github.com/Ian09/CF-NADE hyperparamets in README)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    batch_size = 512\n",
    "    n_iter = 10\n",
    "    look_ahead = 60\n",
    "    lr =0.001  # lr in Adam and SGD, decay in Adadelta\n",
    "    b1 = 0.1 # b1 in Adam, mu in SGD\n",
    "    b2 = 0.001\n",
    "    epsilon = 1e-8\n",
    "    hidden_size_split = 500\n",
    "    hidden_size = [int(x) for x in hidden_size_split]\n",
    "    activation_function = tanh\n",
    "    drop_rate = 0\n",
    "    weight_decay = 0.02\n",
    "    Optimizer = Adam\n",
    "    std = 0\n",
    "    alpha = 1\n",
    "    polyak_mu = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = data_utils.read_dataset('ml-1m')\n",
    "all_contexts = collections.OrderedDict([(user_id, np.zeros(0)) for user_id in users])\n",
    "train_ratings, test_ratings = data_utils.split_ratings(ratings, 0.9, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1409: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../reclab/recommenders/cfnade/cfnade.py:102: UserWarning: Output \"predicted_ratings\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"predicted_ratings\" during training.\n",
      "  optimizer=optimizer)\n"
     ]
    }
   ],
   "source": [
    "recommender = Cfnade(num_users=len(users), num_items=len(items), \n",
    "                     batch_size=512, train_epoch=30, hidden_dim=500, \n",
    "                     learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:625: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/30\n",
      "7/7 [==============================] - 11s - loss: 861.4231 - nade_loss_loss: 394.8461    \n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 8s - loss: 702.8670 - nade_loss_loss: 312.9055     \n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 8s - loss: 615.8297 - nade_loss_loss: 294.0697     \n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 8s - loss: 541.7086 - nade_loss_loss: 276.2223     \n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 8s - loss: 494.6636 - nade_loss_loss: 274.1144     \n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 8s - loss: 439.1696 - nade_loss_loss: 253.8270     \n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 8s - loss: 426.3636 - nade_loss_loss: 268.6168     \n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 8s - loss: 397.6560 - nade_loss_loss: 261.4645     \n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 8s - loss: 416.2427 - nade_loss_loss: 297.0594     \n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 8s - loss: 372.4120 - nade_loss_loss: 266.8013     \n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 8s - loss: 354.1008 - nade_loss_loss: 259.4271     \n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 8s - loss: 338.2145 - nade_loss_loss: 252.4646     \n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 8s - loss: 331.4606 - nade_loss_loss: 253.0964     \n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 8s - loss: 309.1297 - nade_loss_loss: 237.0004     \n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 8s - loss: 320.9603 - nade_loss_loss: 254.0740     \n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 8s - loss: 311.1544 - nade_loss_loss: 248.7250     \n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 8s - loss: 340.3669 - nade_loss_loss: 281.7655     \n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 8s - loss: 310.2434 - nade_loss_loss: 254.9345     \n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 8s - loss: 301.7489 - nade_loss_loss: 249.2446     \n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 8s - loss: 292.6909 - nade_loss_loss: 242.4986     \n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 8s - loss: 292.2617 - nade_loss_loss: 243.9912     \n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 8s - loss: 276.0750 - nade_loss_loss: 229.4658     \n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 8s - loss: 292.6725 - nade_loss_loss: 247.5170     \n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 8s - loss: 284.1786 - nade_loss_loss: 240.3408     \n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 8s - loss: 318.3370 - nade_loss_loss: 275.7191     \n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 8s - loss: 290.4168 - nade_loss_loss: 248.8722     \n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 8s - loss: 284.4207 - nade_loss_loss: 243.8337     \n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 8s - loss: 277.6259 - nade_loss_loss: 237.8512     \n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 8s - loss: 276.7957 - nade_loss_loss: 237.7070     \n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 8s - loss: 263.6127 - nade_loss_loss: 225.1880     \n",
      "Reset takes time:  279.13141894340515\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "recommender.reset(users, items, train_ratings)\n",
    "print('Reset takes time: ', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Prediction takes time:  89.18869543075562\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "t = recommender.dense_predictions\n",
    "t = [(uid, iid, np.zeros(0)) for uid, iid in test_ratings]\n",
    "preds = recommender.predict(t)\n",
    "print('Dense Prediction takes time: ', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.859146964855357\n"
     ]
    }
   ],
   "source": [
    "tot = 0.0\n",
    "num = 0.0\n",
    "for (uid, iid, _), pred in zip(t, preds):\n",
    "    tot += (test_ratings[uid, iid][0] - pred) **2\n",
    "    num += 1\n",
    "print(\"RMSE is\", np.sqrt(tot / num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
