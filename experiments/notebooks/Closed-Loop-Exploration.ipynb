{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustration of Exploration Methods\n",
    "\n",
    "In this notebook, for static recommenders and static user behavior models, we illustrate the effect of different exploration schemes. \n",
    "Specifically, we evaluate the metrics: observed ratings of recommended items, predictive accuracy on recommended items, and predictive accuracy on all items. We may also want to look at diversity metrics, or those used in other papers.\n",
    "\n",
    "For all exploration schemes, we can look at user behavior in the partial-information as well as the full-information case.\n",
    "We may also want to investigate the effect of user interest/model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-Greedy Recommendations\n",
    "\n",
    "Here, the recommendations are top-$N$ with some $\\varepsilon$ randomness scaling from 0 (no randomness) to 1 (completely random). \n",
    "\n",
    "We will look at model updates that don't and that do incorporate the sampling bias into their updates (i.e. by reweighting causally https://arxiv.org/pdf/1706.07639.pdf or http://www.its.caltech.edu/~fehardt/UAI2016WS/papers/Liang.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Recommendations\n",
    "\n",
    "Here, the predicted ratings are transformed into a distribution over items, and items are selected by sampling from it. I.e.\n",
    "$$ \\text{item}~i~\\text{recommended w.p.}~\\frac{(\\widehat r_i)_+^p}{\\sum_j (\\widehat r_j)_+^p} $$\n",
    "\n",
    "We will look at model updates that don't and that do incorporate the sampling bias into their updates (i.e. by reweighting causally https://arxiv.org/pdf/1706.07639.pdf or http://www.its.caltech.edu/~fehardt/UAI2016WS/papers/Liang.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB Recommendations\n",
    "\n",
    "Here, we incorporate some kind of confidence bounds on the predicted ratings, and follow the classic optimistic bandit scheme. In this case, model updates incorporate sample bias/number of samples into their updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
