{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import run_utils\n",
    "\n",
    "sys.path.append('../') \n",
    "import reclab\n",
    "\n",
    "from reclab.recommenders import SLIM, EASE\n",
    "from reclab import data_utils\n",
    "\n",
    "sys.path.append('../tests') \n",
    "import utils\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for metrics\n",
    "\n",
    "def compute_PREC_REC_MAP_NDCG_MRR(N, users, recs, test_ratings):\n",
    "    assert recs.shape[1] >= N\n",
    "    metrics = ['PREC', 'REC', 'MAP', 'NDCG', 'MRR']\n",
    "    res = {key:[] for key in metrics}\n",
    "    test_rating_matrix = np.array(list(test_ratings.keys()))\n",
    "    for user_id, rec in zip(users, recs):\n",
    "        test_matrix = test_rating_matrix[test_rating_matrix[:,0]==user_id,1]\n",
    "        prec, recall, ncdg = precision_recall_ndcg_at_k(N, rec[:N], test_matrix)\n",
    "        MAP, mrr, ncdg = map_mrr_ndcg(rec[:N], test_matrix)\n",
    "        res['PREC'].append(prec)\n",
    "        res['REC'].append(recall)\n",
    "        res['NDCG'].append(ncdg)\n",
    "        res['MAP'].append(MAP)\n",
    "        res['MRR'].append(mrr)\n",
    "    return {key:np.mean(res[key]) for key in metrics}\n",
    "\n",
    "## From \"A troubling analysis\"... \n",
    "### https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/861eafeaba2943458adec22469b147ec492784b6/Conferences/IJCAI/NeuRec_github/eval.py\n",
    "\n",
    "def precision_recall_ndcg_at_k(k, rankedlist, test_matrix):\n",
    "    idcg_k = 0\n",
    "    dcg_k = 0\n",
    "    n_k = k if len(test_matrix) > k else len(test_matrix)\n",
    "    if n_k == 0:\n",
    "        return 0, 0, 0\n",
    "    for i in range(n_k):\n",
    "        idcg_k += 1 / np.log2(i + 2)\n",
    "\n",
    "    b1 = rankedlist\n",
    "    b2 = test_matrix\n",
    "    s2 = set(b2)\n",
    "    hits = [(idx, val) for idx, val in enumerate(b1) if val in s2]\n",
    "    count = len(hits)\n",
    "\n",
    "    for c in range(count):\n",
    "        dcg_k += 1 / np.log2(hits[c][0] + 2)\n",
    "\n",
    "    return float(count / k), float(count / len(test_matrix)), float(dcg_k / idcg_k)\n",
    "\n",
    "\n",
    "def map_mrr_ndcg(rankedlist, test_matrix):\n",
    "    ap = 0\n",
    "    map = 0\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    mrr = 0\n",
    "    if len(test_matrix) == 0:\n",
    "        return 0, 0, 0\n",
    "    for i in range(len(test_matrix)):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    b1 = rankedlist\n",
    "    b2 = test_matrix\n",
    "    s2 = set(b2)\n",
    "    hits = [(idx, val) for idx, val in enumerate(b1) if val in s2]\n",
    "    count = len(hits)\n",
    "\n",
    "    for c in range(count):\n",
    "        ap += (c + 1) / (hits[c][0] + 1)\n",
    "        dcg += 1 / np.log2(hits[c][0] + 2)\n",
    "\n",
    "    if count != 0:\n",
    "        mrr = 1 / (hits[0][0] + 1)\n",
    "\n",
    "    if count != 0:\n",
    "        map = ap / count\n",
    "\n",
    "    return map, mrr, float(dcg / idcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIM\n",
    "\n",
    "In \"A troubling analysis\" (https://arxiv.org/pdf/1911.07698.pdf) Table 12, SLIM achieves the following results on ML 1M.\n",
    "\n",
    "\n",
    "| PREC@5   | REC@5   | MAP@5   | NDCG@5   | MRR@5   | PREC@10   | REC@10   | MAP@10   | NDCG@10   |  MRR@10 |\n",
    "|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n",
    "| 0.4437 |  0.1106 |  0.3692 |  0.1749 |  0.6578 | 0.3813 |  0.1770 |  0.3003 |  0.2321 |  0.667 |\n",
    "\n",
    "\n",
    "In this paper, the dataset is converted into a implicit dataset, so ratings are either 1 or 0. Evaulation was performed by averaging over five different 80/20 train/test splits. (We will just look at a single split below).\n",
    "\n",
    " The [hyperparameters](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/861eafeaba2943458adec22469b147ec492784b6/DL_Evaluation_TOIS_Additional_material.pdf) are set as `l1_ratio=1.89e-5` and `alpha=0.049`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = data_utils.read_dataset('ml-1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ratings.keys():\n",
    "    ratings[key] = (1, ratings[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contexts = collections.OrderedDict([(user_id, np.zeros(0)) for user_id in users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = data_utils.split_ratings(ratings, 0.8, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = SLIM(alpha=0.049, l1_ratio=1.89e-5, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommender.reset(users, items, train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs, _ = recommender.recommend(all_contexts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@5: {'PREC': 0.3241721854304636, 'REC': 0.06738210300791221, 'MAP': 0.43651995952906547, 'NDCG': 0.10395269380895251, 'MRR': 0.45423013245033117}\n",
      "@10: {'PREC': 0.3814238410596027, 'REC': 0.17422638615796424, 'MAP': 0.43883045470150284, 'NDCG': 0.18934677631282193, 'MRR': 0.47939562966466936}\n"
     ]
    }
   ],
   "source": [
    "for N in [5, 10]:\n",
    "    res = compute_PREC_REC_MAP_NDCG_MRR(N, users, recs, test_ratings)\n",
    "    print('@{}:'.format(N), res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EASE\n",
    "\n",
    "In \"A troubling analysis\" (https://arxiv.org/pdf/1911.07698.pdf), EASE achieves the following results on ML 1M\n",
    "\n",
    "\n",
    "\n",
    "| PREC@5   | REC@5   | MAP@5   | NDCG@5   | MRR@5   | PREC@10   | REC@10   | MAP@10   | NDCG@10   |  MRR@10 |\n",
    "|------:|------:|------:|------:|------:|------:|------:|------:|------:|------:|\n",
    "| 0.4360  | 0.1073  | 0.3608  | 0.1697  | 0.6475 | 0.3745  | 0.1731  | 0.2923  | 0.2259  | 0.65| \n",
    " \n",
    "In this paper, the dataset is converted into a implicit dataset, so ratings are either 1 or 0. Evaulation was performed by averaging over five different 80/20 train/test splits. (We will just look at a single split below).\n",
    "\n",
    "The [hyperparameters](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/861eafeaba2943458adec22469b147ec492784b6/DL_Evaluation_TOIS_Additional_material.pdf) are set as `lam=1.25e3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items, ratings = data_utils.read_dataset('ml-1m')\n",
    "for key in ratings.keys():\n",
    "    ratings[key] = (1, ratings[key][1])\n",
    "all_contexts = collections.OrderedDict([(user_id, np.zeros(0)) for user_id in users])\n",
    "train_ratings, test_ratings = data_utils.split_ratings(ratings, 0.8, shuffle=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = EASE(lam=1.25e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "recommender.reset(users, items, train_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs, _ = recommender.recommend(all_contexts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@5: {'PREC': 0.32466887417218543, 'REC': 0.06821147096556184, 'MAP': 0.4329348325974981, 'NDCG': 0.10444392654073191, 'MRR': 0.4466197571743929}\n",
      "@10: {'PREC': 0.38415562913907286, 'REC': 0.17628998806236673, 'MAP': 0.4373985546772671, 'NDCG': 0.19097229149913342, 'MRR': 0.4719510801009146}\n"
     ]
    }
   ],
   "source": [
    "for N in [5, 10]:\n",
    "    res = compute_PREC_REC_MAP_NDCG_MRR(N, users, recs, test_ratings)\n",
    "    print('@{}:'.format(N), res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
